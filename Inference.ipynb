{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, PNDMScheduler\n",
    "from transformers import T5EncoderModel, T5TokenizerFast\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/blob/v-yuancwang/AudioEditingModel/Diffusion_GEN/checkpoint-16000\"\n",
    "model_path = \"/blob/v-yuancwang/AudioEditingModel/Diffusion_SG/checkpoint-42000\"\n",
    "vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\")\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = T5EncoderModel.from_pretrained(model_path, subfolder=\"text_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda:2\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mel_path = \"/home/v-yuancwang/AudioEditing/test_generate\"\n",
    "save_fig_path = \"/home/v-yuancwang/AudioEditing/figs\"\n",
    "# mel_src = np.load(\"/blob/v-yuancwang/audio_editing_data/sr/mel/LsCTeF3TbAU.npy\")\n",
    "# mel_tgt = np.load(\"/blob/v-yuancwang/audio_editing_data/audiocaps/mel/LsCTeF3TbAU.npy\")\n",
    "text = \"Generate: jazz music\"\n",
    "# np.save(os.path.join(save_mel_path, text.replace(\":\", \" \") + \"_src.npy\"), mel_src)\n",
    "# np.save(os.path.join(save_mel_path, text.replace(\":\", \" \") + \"_tgt.npy\"), mel_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mel_src)\n",
    "plt.savefig(fname=os.path.join(save_fig_path, text.replace(\":\", \" \") + \"_src.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mel_tgt)\n",
    "plt.savefig(fname=os.path.join(save_fig_path, text.replace(\":\", \" \") + \"_tgt.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [text]\n",
    "text_input = tokenizer(prompt, max_length=tokenizer.model_max_length, truncation=True, padding=\"do_not_pad\", return_tensors=\"pt\")\n",
    "text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * 1, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
    "print(text_embeddings.shape)\n",
    "print(uncond_embeddings.shape)\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_src = torch.Tensor(np.array([[mel_src]])).to(torch_device)\n",
    "latents_src = vae.encode(latents_src).latent_dist.sample()\n",
    "print(latents_src.shape, latents_src.dtype, latents_src.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler\n",
    "num_inference_steps = 100\n",
    "# scheduler = DDIMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "scheduler = PNDMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "scheduler.set_timesteps(num_inference_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 7.5\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "latents = torch.randn((1, 4, 10, 78)).to(torch_device)\n",
    "# latents_src_input = torch.cat([latents_src] * 2)\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        # noise_pred = unet(torch.cat((latent_model_input, latents_src_input), dim=1), t, encoder_hidden_states=text_embeddings).sample\n",
    "        # noise_pred = unet(torch.cat((latent_model_input, latent_model_input), dim=1), t, encoder_hidden_states=text_embeddings).sample\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "    \n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latents_out = latents_src + latents\n",
    "latents_out = latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    res = vae.decode(latents_out).sample\n",
    "res = res.cpu().numpy()[0,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(save_mel_path, text.replace(\":\", \" \") + \".npy\"), res)\n",
    "plt.imshow(res)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig(fname=os.path.join(save_fig_path, text.replace(\":\", \" \") + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ee27c2a92f1fff136d50aad92bfca040aea835edd9bacd8b4c989384ca9eab3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
